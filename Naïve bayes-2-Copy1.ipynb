{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d2a3537-0be5-4707-9730-a5f48018f4c7",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ecdd14-75cd-4289-9a7a-8a10d439f78b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    This problem involves conditional probability. We want to find the probability that an employee is a smoker given that they use the health insurance plan. Let's define the events:\n",
    "\n",
    "- Event S: Employee is a smoker.\n",
    "- Event H: Employee uses the health insurance plan.\n",
    "\n",
    "We are given:\n",
    "- \\( P(H) = 0.70 \\) (probability that an employee uses the health insurance plan).\n",
    "- \\( P(S|H) = 0.40 \\) (probability that an employee is a smoker given that they use the health insurance plan).\n",
    "\n",
    "We want to find:\n",
    "- \\( P(S|H) \\) (probability that an employee is a smoker given that they use the health insurance plan).\n",
    "\n",
    "By definition of conditional probability:\n",
    "\\[ P(S|H) = \\frac{P(S \\cap H)}{P(H)} \\]\n",
    "\n",
    "We know that \\( P(S \\cap H) \\) is the probability that an employee is both a smoker and uses the health insurance plan. We are not given this directly, but we can calculate it using the information provided:\n",
    "\n",
    "\\[ P(S \\cap H) = P(S|H) \\cdot P(H) = 0.40 \\cdot 0.70 \\]\n",
    "\n",
    "Now we can substitute this value into the conditional probability formula:\n",
    "\n",
    "\\[ P(S|H) = \\frac{P(S \\cap H)}{P(H)} = \\frac{0.40 \\cdot 0.70}{0.70} = 0.40 \\]\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that they use the health insurance plan is 0.40, or 40%.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153ed86-2bf5-4753-af5d-2496104c854f",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1cdd3-ab3c-47f9-a611-53c07bc90154",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm, used for classification tasks, particularly in text and document classification. They differ in how they handle the features (input data) and the assumptions they make about the data distribution.\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - **Features:** Bernoulli Naive Bayes is used when the features are binary or boolean in nature, i.e., they are present or absent.\n",
    "   - **Assumption:** It assumes that the presence or absence of a feature is relevant to the classification, while ignoring the frequency or count of the feature.\n",
    "   - **Use Cases:** It's commonly used for text classification tasks where the focus is on the presence or absence of words in a document. For example, spam detection, sentiment analysis, or document categorization.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - **Features:** Multinomial Naive Bayes is used when the features are discrete and represent counts or frequencies. This is often the case in text classification, where features can be word counts or term frequencies.\n",
    "   - **Assumption:** It assumes that features are generated from a multinomial distribution, which means it considers the frequency of occurrences of different features.\n",
    "   - **Use Cases:** Multinomial Naive Bayes is well-suited for tasks where the frequency of words or terms in documents matters. For example, topic classification, spam filtering, and other tasks where word frequency information is important.\n",
    "\n",
    "In summary, the main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the nature of the features they handle and the assumptions they make about the data distribution. Bernoulli Naive Bayes is used for binary features where presence or absence matters, while Multinomial Naive Bayes is used for discrete features that represent counts or frequencies, often in the context of text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c40209-e457-4efb-8bab-1d6d13a2187a",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ad1e7-299b-46fd-bcac-da8a7ff73c07",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Bernoulli Naive Bayes handles missing values by considering them as a separate category or by ignoring them during classification, depending on how the algorithm is implemented and the approach taken to handle missing data. The specific approach may vary based on the implementation and the problem at hand. Here are a couple of common ways Bernoulli Naive Bayes can handle missing values:\n",
    "\n",
    "1. **Treating Missing Values as a Separate Category:**\n",
    "   In some cases, missing values are treated as their own category when using Bernoulli Naive Bayes. This means that a missing value for a feature is considered a unique state, just like having a value of 0 or 1. When calculating probabilities, the algorithm includes this missing category as part of the calculations. This approach assumes that the fact that a value is missing might be informative and could influence the classification.\n",
    "\n",
    "2. **Ignoring Missing Values:**\n",
    "   Alternatively, Bernoulli Naive Bayes can be implemented to simply ignore instances with missing values during the calculation of probabilities. This is often the case when missing values are treated as noise or when there's no clear reason to believe that the missing values provide meaningful information for the classification task. In this approach, the algorithm would exclude instances with missing values from calculations and predictions.\n",
    "\n",
    "The choice of approach depends on the context of the problem, the nature of the missing data, and the goals of the classification task. It's important to note that handling missing values is a crucial step in any classification algorithm, including Bernoulli Naive Bayes, to ensure accurate and reliable predictions. Depending on the library or framework you are using, there might be default behavior for handling missing values, or you might need to implement a specific strategy yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338729e0-ffd0-4aa6-a683-5e429b4b8b2d",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93128962-28d3-4ea6-a898-6b1d18f500f2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes continuous features are distributed according to a Gaussian (normal) distribution within each class. While it's often used for binary and two-class classification problems, it can also be extended for multi-class classification by applying the algorithm to each class in a one-vs-all (OvA) or one-vs-one (OvO) fashion.\n",
    "\n",
    "Here's how Gaussian Naive Bayes can be used for multi-class classification:\n",
    "\n",
    "**One-vs-All (OvA) Approach:**\n",
    "In the OvA approach, you create a separate Gaussian Naive Bayes classifier for each class, treating it as the positive class, and all other classes as the negative class. During training, you calculate the mean and variance of each feature for each class. When making predictions, you use all the trained classifiers and assign the instance to the class with the highest calculated posterior probability.\n",
    "\n",
    "**One-vs-One (OvO) Approach:**\n",
    "In the OvO approach, you create a Gaussian Naive Bayes classifier for each pair of classes. So, if you have \\(N\\) classes, you would create \\(N \\times (N-1) / 2\\) classifiers. During training, each classifier is trained on instances from the two classes it represents. When making predictions, you apply all the trained classifiers and assign the instance to the class that wins the most binary classification competitions.\n",
    "\n",
    "Both approaches allow you to extend Gaussian Naive Bayes to multi-class problems. However, keep in mind that Naive Bayes, including Gaussian Naive Bayes, has certain assumptions that might not hold in complex real-world scenarios. It might not always perform as well as more sophisticated algorithms like support vector machines, random forests, or neural networks for multi-class classification tasks with high-dimensional data and intricate relationships between features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbab3e-b6ba-4ee7-bab9-c748b15b1454",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a13095-7aed-41fe-9677-38ed8ad1596e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
